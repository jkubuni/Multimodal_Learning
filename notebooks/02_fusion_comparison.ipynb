{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bb39c58",
   "metadata": {
    "id": "0bb39c58"
   },
   "source": [
    "# Fusion Strategy Comparison\n",
    "This notebook compares different fusion strategies for multimodal learning (RGB + LiDAR).\n",
    "Strategies evaluated:\n",
    "1.  **Late Fusion**: Process modalities separately and combine at the end.\n",
    "2.  **Intermediate Fusion**: Combine features at an intermediate layer using:\n",
    "    *   Concatenation\n",
    "    *   Addition\n",
    "    *   Multiplication\n",
    "\n",
    "The models are defined in `src/models.py` and training utilities in `src/training.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c69ee",
   "metadata": {},
   "source": [
    "## Setup\n",
    "### Drive\n",
    "This cell tries to use the data and files from google drive.\n",
    "The Paths may be changed for different drive file structures/layouts. This assumes a structur of `MyDrive/extended_assessments/Multimodal_Learning/{repo contents}` and the gdrive to be mounted at `/gdrive`.\n",
    "If that does not work we fallback to local files (which of course only works, when there is a local copy of the files and code. This fallback was required, since KDE, the desktop environment I am using, has no support for accessing google drive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efqZsUQyCyXJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35631,
     "status": "ok",
     "timestamp": 1767036129010,
     "user": {
      "displayName": "Jonas Kubeler",
      "userId": "00495256255093359137"
     },
     "user_tz": -60
    },
    "id": "efqZsUQyCyXJ",
    "outputId": "84d03380-9b21-4f7c-b3eb-7f5d6517176b"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    print(\"Mounted Google Drive\")\n",
    "    DATA_DIR = Path('/gdrive/MyDrive/extended_assessments/Multimodal_Learning/data')\n",
    "    sys.path.append(os.path.abspath('/gdrive/MyDrive/extended_assessments/Multimodal_Learning'))\n",
    "except:\n",
    "    print(\"Running locally\")\n",
    "    DATA_DIR = Path('../data')\n",
    "    sys.path.append(os.path.abspath('../.'))\n",
    "print(f\"Using {DATA_DIR} as data source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2d0629",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "The `src.datasets`, `src.models` and `src.training` imports are from the files in `src/`. We need the `src.` prefix, since otherwise python confuses our own datasets file with the huggingface datasets library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0c0c51",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19441,
     "status": "ok",
     "timestamp": 1767036155394,
     "user": {
      "displayName": "Jonas Kubeler",
      "userId": "00495256255093359137"
     },
     "user_tz": -60
    },
    "id": "bf0c0c51",
    "outputId": "7492b125-3933-4523-fe55-758d808ebcd0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "# Import dataset and models from src\n",
    "import sys\n",
    "import os\n",
    "from src.datasets import CubesAndShperesDataset\n",
    "from src.models import LateFusionModel, IntermediateFusionModel\n",
    "from src.training import train_model, evaluate_model, count_parameters\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "WANDB_ENTITY = \"jan-kubeler-hpi\"\n",
    "WANDB_PROJECT = \"clip-extended-assessment\"\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a43f6",
   "metadata": {},
   "source": [
    "### Reproducability\n",
    "\n",
    "This function is used to set all random seeds for reproducable results.\n",
    "I copied it from the notebooks in https://github.com/andandandand/practical-computer-vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178caf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=51):\n",
    "    \"\"\"\n",
    "    Set seeds for complete reproducibility across all libraries and operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Random seed value\n",
    "    \"\"\"\n",
    "    # Set environment variables before other imports\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch CPU\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # PyTorch GPU (all devices)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "        # CUDA deterministic operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # PyTorch deterministic algorithms (may impact performance)\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except RuntimeError:\n",
    "        # Some operations don't have deterministic implementations\n",
    "        print(\"Warning: Some operations may not be deterministic\")\n",
    "\n",
    "    print(f\"All random seeds set to {seed} for reproducibility\")\n",
    "\n",
    "\n",
    "set_seeds(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5eeda8",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "The dataset class used is `CubesAndShperesDataset` from `src/datasets`. A noteworthy difference to the dataset from the NVIDIA Lab is, that I normalized the LiDAR values to be between 0 and 1 and not 0 and 50.\n",
    "\n",
    "Since the CPU compute on google collab seems quite low and building the dataset took quite some time, I added the option to export the datasets and load them again, reducing the creation compute overhead. The first cell below tries to load the precomputed dataset from `dataset_precomputed.pkl`. The second cell below builds the datasets from scratch and the third cell below exports the created datasets into the `dataset_precomputed.pkl` file.\n",
    "\n",
    "**If the precomputed file is available, only run the first of the three following cells!**\n",
    "Otherwise this will take up to three hours!\n",
    "\n",
    "The splitting logic is aligned with the NVIDIA Lab, where we take 10 Batches of 32 Samples per class for validation. This splitting is not very sophisticated, but I wanted to maintain alignment with the original experiments, since the final evaluation uses the Lab numbers as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TxfpfkbejlL3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 3328,
     "status": "error",
     "timestamp": 1767036189632,
     "user": {
      "displayName": "Jonas Kubeler",
      "userId": "00495256255093359137"
     },
     "user_tz": -60
    },
    "id": "TxfpfkbejlL3",
    "outputId": "cf5b3913-b919-456b-e80d-0e64d0849ffb"
   },
   "outputs": [],
   "source": [
    "# load precomputed dataset\n",
    "import pickle\n",
    "with open(\"/gdrive/MyDrive/extended_assessments/Multimodal_Learning/notebooks/dataset_precomputed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "train_indices = data[\"train_indices\"]\n",
    "val_indices = data[\"val_indices\"]\n",
    "train_dataset = data[\"train_dataset\"]\n",
    "val_dataset = data[\"val_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3OO7NEyWjmQY",
   "metadata": {
    "id": "3OO7NEyWjmQY"
   },
   "outputs": [],
   "source": [
    "# Build Datasets\n",
    "full_dataset = CubesAndShperesDataset(DATA_DIR)\n",
    "\n",
    "# Split into Train and Validation (No Test)\n",
    "# Logic from 05_Assessment.ipynb: Last VALID_BATCHES * BATCH_SIZE are validation\n",
    "\n",
    "total_len = len(full_dataset)\n",
    "n_classes = 2\n",
    "samples_per_class = total_len // n_classes\n",
    "\n",
    "VALID_BATCHES = 10\n",
    "valid_samples_per_class = VALID_BATCHES * BATCH_SIZE\n",
    "train_samples_per_class = samples_per_class - valid_samples_per_class\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "for i in range(n_classes):\n",
    "    start_idx = i * samples_per_class\n",
    "    train_indices.extend(range(start_idx, start_idx + train_samples_per_class))\n",
    "    val_indices.extend(range(start_idx + train_samples_per_class, start_idx + samples_per_class))\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558dcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset_precomputed.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"train_indices\": train_indices,\n",
    "        \"val_indices\": val_indices,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"val_dataset\": val_dataset\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f83fa7a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f83fa7a",
    "outputId": "8b13e312-cceb-47e4-bd1c-9aed0e4b7a09"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Total samples: {total_len}\")\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fe9a02",
   "metadata": {
    "id": "b2fe9a02"
   },
   "source": [
    "## Model Architectures\n",
    "\n",
    "The models `LateFusionModel` and `IntermediateFusionModel` are defined in `src/models.py` and imported above.\n",
    "\n",
    "### Late Fusion Model\n",
    "Process modalities separately and combine outputs.\n",
    "\n",
    "### Intermediate Fusion Model\n",
    "Combine feature maps at an intermediate layer. We explore three variations:\n",
    "*   Concatenation\n",
    "*   Addition\n",
    "*   Hadamard Product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f654911",
   "metadata": {
    "id": "7f654911"
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Here is the place to define the Hyperparameters (LRs, using a CosineAnnealing Scheduler and the fusion strategies). This code performs a simple grid search over the provided parameters. (Intermediate) Results are logged to W&B. Some final metrics are plotted in the end.\n",
    "\n",
    "The current implementation runs full training on the full dataset. Since that only took about 2 minutes per run, I think that is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab598fa",
   "metadata": {
    "id": "fab598fa"
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# Hyperparameters to test\n",
    "learning_rates = [1e-4, 1e-5, 1e-6]\n",
    "use_schedulers = [True, False]\n",
    "\n",
    "# Define models to test\n",
    "# Format: (Name, ID, ModelClass, InitArgs)\n",
    "strategies = [\n",
    "    (\"Late Fusion\", \"late-fusion\", LateFusionModel, {}),\n",
    "    (\"Intermediate (Concat)\", \"intermediate-concat\", IntermediateFusionModel, {'fusion_type': 'concat'}),\n",
    "    (\"Intermediate (Add)\", \"intermediate-add\", IntermediateFusionModel, {'fusion_type': 'add'}),\n",
    "    (\"Intermediate (Multiply)\", \"intermediate-multiply\", IntermediateFusionModel, {'fusion_type': 'multiply'}),\n",
    "]\n",
    "best_val_acc = 0.0\n",
    "best_model_info = None\n",
    "for name, strategy_id, model_cls, model_kwargs in strategies:\n",
    "    for lr in learning_rates:\n",
    "        for use_sched in use_schedulers:\n",
    "            sched_str = \"cosine\" if use_sched else \"constant\"\n",
    "            run_name = f\"{strategy_id}_lr{lr}_{sched_str}\"\n",
    "\n",
    "            print(f\"\\n=== Training {name} | LR: {lr} | Scheduler: {sched_str} ===\")\n",
    "\n",
    "            wandb.init(project=WANDB_PROJECT, name=run_name, entity=WANDB_ENTITY, config={\n",
    "                \"fusion_strategy\": strategy_id,\n",
    "                \"batch_size\": BATCH_SIZE,\n",
    "                \"learning_rate\": lr,\n",
    "                \"scheduler\": sched_str,\n",
    "                \"epochs\": EPOCHS\n",
    "            })\n",
    "\n",
    "            # Initialize model\n",
    "            model = model_cls(**model_kwargs).to(DEVICE)\n",
    "            model_params = count_parameters(model)\n",
    "\n",
    "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            scheduler = None\n",
    "            if use_sched:\n",
    "                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "            # Train\n",
    "            train_loss, val_loss, val_acc, train_time = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=EPOCHS, scheduler=scheduler, use_wandb=True)\n",
    "\n",
    "            # Evaluate on Validation Set (since we don't have a separate test set)\n",
    "            final_acc, final_f1, _, _ = evaluate_model(model, val_loader)\n",
    "\n",
    "            results[run_name] = {\n",
    "                'Validation Loss': val_loss[-1],\n",
    "                'Parameters': model_params,\n",
    "                'Training Time (s)': train_time,\n",
    "                'Final Accuracy': final_acc,\n",
    "                'Final F1': final_f1\n",
    "            }\n",
    "\n",
    "            print(f\"Final Val Metrics: Acc={final_acc:.4f}, F1={final_f1:.4f}\")\n",
    "            \n",
    "            if final_acc > best_val_acc:\n",
    "                best_val_acc = final_acc\n",
    "                best_model_info = {\n",
    "                    \"name\": name,\n",
    "                    \"strategy_id\": strategy_id,\n",
    "                    \"model_cls\": model_cls,\n",
    "                    \"model_kwargs\": model_kwargs,\n",
    "                    \"learning_rate\": lr,\n",
    "                    \"scheduler\": sched_str,\n",
    "                    \"accuracy\": final_acc,\n",
    "                    \"f1_score\": final_f1\n",
    "                }\n",
    "                torch.save(model.state_dict(), f\"/gdrive/MyDrive/extended_assessments/Multimodal_Learning/checkpoints/best_fusion_model.pth\")\n",
    "                with open(\"/gdrive/MyDrive/extended_assessments/Multimodal_Learning/checkpoints/best_fusion_model_info.txt\", \"wb\") as f:\n",
    "                    f.write(str(best_model_info).encode())\n",
    "\n",
    "            wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ae050",
   "metadata": {
    "id": "3d5ae050"
   },
   "source": [
    "## Comparison and Analysis\n",
    "\n",
    "For in depth analysis, check out W&B. Here we plot the final accuracies and Training times of the different hyperparameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157396a5",
   "metadata": {
    "id": "157396a5"
   },
   "outputs": [],
   "source": [
    "# Create Comparison Table\n",
    "df_results = pd.DataFrame(results).T\n",
    "print(df_results)\n",
    "\n",
    "# Bar plot of Final Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_results['Final Accuracy'].plot(kind='bar')\n",
    "plt.title('Final Validation Accuracy by Strategy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of Training Time\n",
    "plt.figure(figsize=(12, 6))\n",
    "df_results['Training Time (s)'].plot(kind='bar', color='orange')\n",
    "plt.title('Training Time by Strategy')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f23303",
   "metadata": {
    "id": "89f23303"
   },
   "source": [
    "### Analysis\n",
    "\n",
    "#### Theoretical Thoughts\n",
    "\n",
    "The main difference between Late and Intermediate Fusion is the level of independence of the two modalities. In Late Fusion the model has to focus more on the two seperate modalities, potentially allowing for better, more detailed, understanding of both, and only combines them in the end. In real-world applications this also allows for better reuse of pretrained models, since they can just be ''plugged in'' and used. \n",
    "\n",
    "The Intermediate Fusion on the other hand is used for for better combination of both modalities. Since we combine them earlier in the intermediate architectures, the model is more capable of findeing correlations between the two modalities, which theoretically should lead for better performance, when there are strong correlations in the datapoints of the different modalities.\n",
    "* The **Concatination** approach has the benefit of fully maintaining the features of both modalities, but comes at the cost of greater dimensionality, resulting in higher parameter count and longer training.\n",
    "* The **Addition** approach keeps the dimensionality lower and puts the same importance to all features in both modalities.\n",
    "* The **Multiplication** approach also keeps the dimensionality down, but puts more importance onto features present in both modalities. Since multiplications results in a small product when at least one factor is small, but in a greater product if both factors are larger. Ideally this leads to more sophisticated alignment of the two dimensions.\n",
    "\n",
    "#### Practical Results\n",
    "| Metric        | Late Fusion | IF-Add | IF-Multiply | IF-Concat |\n",
    "|---------------|-------------|--------|-------------|-----------|\n",
    "| Val Loss      |             |        |             |           |\n",
    "| Parameters    |             |        |             |           |\n",
    "| Training Time |             |        |             |           |\n",
    "| GPU Memory    |             |        |             |           |\n",
    "\n",
    "Training time here covers a full 15 epochs training run on a T4 Colab system."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
