{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "850b1d1f",
   "metadata": {},
   "source": [
    "# Final Assessment: Cross-Modal Projection\n",
    "This notebook implements the final assessment task: classifying RGB images using a classifier trained *only* on LiDAR data.\n",
    "This is achieved by:\n",
    "1.  **LiDAR Classifier**: Training a classifier on LiDAR range images.\n",
    "2.  **CILP (Contrastive Image-LiDAR Pretraining)**: Aligning RGB and LiDAR embeddings using a contrastive loss (CLIP-style).\n",
    "3.  **Cross-Modal Projector**: Training a projector to map aligned RGB embeddings to the LiDAR classifier's embedding space.\n",
    "4.  **Zero-Shot/Proxy Classification**: Classifying RGB images by projecting them to the LiDAR space and using the frozen LiDAR classifier.\n",
    "\n",
    "The models are defined in `src/models.py` and training utilities in `src/training.py`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8500954",
   "metadata": {},
   "source": [
    "## Setup\n",
    "The usual drive, import and reproducability setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb67f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/gdrive')\n",
    "    print(\"Mounted Google Drive\")\n",
    "    DATA_DIR = Path('/gdrive/MyDrive/extended_assessments/Multimodal_Learning/data')\n",
    "    sys.path.append(os.path.abspath('/gdrive/MyDrive/extended_assessments/Multimodal_Learning'))\n",
    "except:\n",
    "    print(\"Running locally\")\n",
    "    DATA_DIR = Path('../data')\n",
    "    sys.path.append(os.path.abspath('../.'))\n",
    "print(f\"Using {DATA_DIR} as data source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09abfd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "from src.datasets import CubesAndShperesDataset\n",
    "from src.models import Embedder, EmbedderStrided, ContrastivePretraining, Projector, RGB2LiDARClassifier\n",
    "from src.training import train_classifier, train_cilp, train_projector\n",
    "from src.visualization import plot_losses\n",
    "\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 15\n",
    "WANDB_ENTITY = \"jan-kubeler-hpi\"\n",
    "WANDB_PROJECT = \"clip-extended-assessment\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26482a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=51):\n",
    "    \"\"\"\n",
    "    Set seeds for complete reproducibility across all libraries and operations.\n",
    "\n",
    "    Args:\n",
    "        seed (int): Random seed value\n",
    "    \"\"\"\n",
    "    # Set environment variables before other imports\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "\n",
    "    # Python random module\n",
    "    random.seed(seed)\n",
    "\n",
    "    # NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch CPU\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # PyTorch GPU (all devices)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
    "\n",
    "        # CUDA deterministic operations\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # PyTorch deterministic algorithms (may impact performance)\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except RuntimeError:\n",
    "        # Some operations don't have deterministic implementations\n",
    "        print(\"Warning: Some operations may not be deterministic\")\n",
    "\n",
    "    print(f\"All random seeds set to {seed} for reproducibility\")\n",
    "\n",
    "\n",
    "set_seeds(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e0fa4",
   "metadata": {},
   "source": [
    "## Dataset and DataLoader\n",
    "Same as for the previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b154780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load precomputed dataset\n",
    "import pickle\n",
    "with open(\"/gdrive/MyDrive/extended_assessments/Multimodal_Learning/notebooks/dataset_precomputed.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "train_indices = data[\"train_indices\"]\n",
    "val_indices = data[\"val_indices\"]\n",
    "train_dataset = data[\"train_dataset\"]\n",
    "val_dataset = data[\"val_dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedaec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Datasets\n",
    "full_dataset = CubesAndShperesDataset(DATA_DIR)\n",
    "\n",
    "# Split into Train and Validation (No Test)\n",
    "# Logic from 05_Assessment.ipynb: Last VALID_BATCHES * BATCH_SIZE are validation\n",
    "\n",
    "total_len = len(full_dataset)\n",
    "n_classes = 2\n",
    "samples_per_class = total_len // n_classes\n",
    "\n",
    "VALID_BATCHES = 10\n",
    "valid_samples_per_class = VALID_BATCHES * BATCH_SIZE\n",
    "train_samples_per_class = samples_per_class - valid_samples_per_class\n",
    "\n",
    "train_indices = []\n",
    "val_indices = []\n",
    "for i in range(n_classes):\n",
    "    start_idx = i * samples_per_class\n",
    "    train_indices.extend(range(start_idx, start_idx + train_samples_per_class))\n",
    "    val_indices.extend(range(start_idx + train_samples_per_class, start_idx + samples_per_class))\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "val_dataset = Subset(full_dataset, val_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f66a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"dataset_precomputed.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"train_indices\": train_indices,\n",
    "        \"val_indices\": val_indices,\n",
    "        \"train_dataset\": train_dataset,\n",
    "        \"val_dataset\": val_dataset\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628b6e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Total samples: {total_len}\")\n",
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Val size: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07093a74",
   "metadata": {},
   "source": [
    "## 2. Train LiDAR Classifier (Pre-requisite)\n",
    "We first need a pre-trained LiDAR classifier to act as our target for the cross-modal projection. We will use the `EmbedderStrided` architecture as it was identified as the best performing downsampling method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaabc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lidar_cnn = EmbedderStrided(in_ch=4, out_dim=2).to(device)\n",
    "optimizer_lidar = Adam(lidar_cnn.parameters(), lr=0.0005)\n",
    "criterion_lidar = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train LiDAR Classifier\n",
    "lidar_cnn = train_classifier(lidar_cnn, train_loader, optimizer_lidar, criterion_lidar, epochs=EPOCHS, device=device)\n",
    "\n",
    "# Freeze LiDAR CNN\n",
    "for param in lidar_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "lidar_cnn.eval()\n",
    "print(\"LiDAR Classifier trained and frozen.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ad4e8d",
   "metadata": {},
   "source": [
    "## 3. Contrastive Image-LiDAR Pre-training (CILP)\n",
    "We will train a CILP model to align RGB and LiDAR embeddings. We use `EmbedderStrided` for both modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5860157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CILP_EMB_SIZE = 200\n",
    "cilp_model = ContrastivePretraining(embedding_size=CILP_EMB_SIZE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700249e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"clip-extended-assessment\", name=\"cilp-training\", entity=\"jan-kubeler-hpi\", config={\n",
    "    \"task\": \"CILP\",\n",
    "    \"embedding_size\": CILP_EMB_SIZE,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"epochs\": 15\n",
    "})\n",
    "\n",
    "optimizer_cilp = Adam(cilp_model.parameters(), lr=0.0001)\n",
    "train_losses, valid_losses = train_cilp(cilp_model, train_loader, val_loader, optimizer_cilp, epochs=EPOCHS, device=device, use_wandb=True)\n",
    "\n",
    "# Plot\n",
    "plot_losses(train_losses, valid_losses, title=\"CILP Training Loss\")\n",
    "\n",
    "# Freeze CILP\n",
    "for param in cilp_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b55edb",
   "metadata": {},
   "source": [
    "## 4. Cross-Modal Projector\n",
    "Train a projector to map RGB embeddings (from CILP) to LiDAR embeddings (from the pre-trained LiDAR classifier)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321d7059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projector maps CILP RGB embeddings (200) to LiDAR Classifier embeddings (100)\n",
    "projector = Projector(input_dim=CILP_EMB_SIZE, output_dim=100).to(device)\n",
    "optimizer_proj = Adam(projector.parameters(), lr=0.0005)\n",
    "criterion_proj = nn.MSELoss()\n",
    "\n",
    "proj_losses = train_projector(projector, cilp_model, lidar_cnn, train_loader, optimizer_proj, criterion_proj, epochs=EPOCHS, device=device, use_wandb=True)\n",
    "\n",
    "plot_losses(proj_losses, title=\"Projector Training Loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b222e1",
   "metadata": {},
   "source": [
    "## 5. Final RGB2LiDAR Classifier\n",
    "Combine the pieces to classify RGB images using the LiDAR classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9abc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = RGB2LiDARClassifier(cilp_model.img_embedder, projector, lidar_cnn).to(device)\n",
    "\n",
    "final_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "similarity_matrix = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Calculate accuracy\n",
    "    for batch in val_loader:\n",
    "        rgb, _, label = batch\n",
    "        rgb, label = rgb.to(device), label.to(device).long().squeeze()\n",
    "        \n",
    "        output = final_model(rgb)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "\n",
    "    # Calculate similarity matrix on one batch for visualization\n",
    "    rgb_batch, lidar_batch, _ = next(iter(val_loader))\n",
    "    rgb_batch, lidar_batch = rgb_batch.to(device), lidar_batch.to(device)\n",
    "    logits_per_img, _ = cilp_model(rgb_batch, lidar_batch)\n",
    "    similarity_matrix = logits_per_img.cpu().numpy()\n",
    "\n",
    "acc = 100 * correct / total\n",
    "print(f\"Final Accuracy on RGB images (using LiDAR proxy): {acc:.2f}%\")\n",
    "\n",
    "wandb.log({\n",
    "    \"final_accuracy\": acc,\n",
    "    \"similarity_matrix\": wandb.Image(similarity_matrix, caption=\"CILP Similarity Matrix (Validation Batch)\")\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82378207",
   "metadata": {},
   "source": [
    "## 6. Save Model Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d9eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'cilp_model': cilp_model.state_dict(),\n",
    "    'projector': projector.state_dict(),\n",
    "    'final_model': final_model.state_dict()\n",
    "}, '../results/assessment_checkpoint.pt')\n",
    "print(\"Model saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
